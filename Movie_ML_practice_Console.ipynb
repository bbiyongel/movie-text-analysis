{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie ML practice Console",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNckRCeT1GT9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "78425783-8338-4999-f418-506845556354"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B0T9vdpW8CY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2efafb0c-9c7e-4f49-a31f-f6563bbb151d"
      },
      "source": [
        "cd \"/content/drive/My Drive\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GyvGvOacDYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls -al"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QneczCKI1XIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install konlpy\n",
        "!apt -qq -y install fonts-nanum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loLYFra8W7HP",
        "colab_type": "code",
        "outputId": "bc8dd5b3-a6be-46c6-ce62-d4f240815e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import json\n",
        "import os\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from pprint import pprint\n",
        "from konlpy.tag import Okt\n",
        "from matplotlib import font_manager, rc\n",
        "\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def read_data(filename):\n",
        "    with open(filename, 'r') as buffer:\n",
        "        data = [line.split('\\t') for line in buffer.read().splitlines()]\n",
        "        data = data[1:]\n",
        "    return data\n",
        "\n",
        "\n",
        "def tokenize(document):\n",
        "    return ['/'.join(tag) for tag in okt.pos(document, norm=True, stem=True)]\n",
        "\n",
        "\n",
        "def showGraph(data):\n",
        "    font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "    font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
        "    rc('font', family=font_name)\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    data.plot(50)\n",
        "\n",
        "\n",
        "def term_frequency(doc):\n",
        "    return [doc.count(word) for word in selected_words]\n",
        "\n",
        "\n",
        "okt = Okt()\n",
        "train_data = read_data('ratings_train.txt')\n",
        "test_data = read_data('ratings_test.txt')\n",
        "\n",
        "if os.path.isfile('train_docs.json'):\n",
        "    with open('train_docs.json') as fileBuffer:\n",
        "        train_docs = json.load(fileBuffer)\n",
        "    with open('test_docs.json') as fileBuffer:\n",
        "        test_docs = json.load(fileBuffer)\n",
        "else:\n",
        "    train_docs = [(tokenize(row[1]), row[2]) for row in train_data]\n",
        "    test_docs = [(tokenize(row[1]), row[2]) for row in test_data]\n",
        "\n",
        "    with open('train_docs.json', 'w', encoding=\"utf-8\") as file:\n",
        "        json.dump(train_docs, file, ensure_ascii=False, indent=\"\\t\")\n",
        "    with open('test_docs.json', 'w', encoding=\"utf-8\") as file:\n",
        "        json.dump(test_docs, file, ensure_ascii=False, indent=\"\\t\")\n",
        "\n",
        "tokens = [token for data in train_docs for token in data[0]]\n",
        "text = nltk.Text(tokens, name='NMSC')\n",
        "# showGraph(text)\n",
        "\n",
        "selected_words = [f[0] for f in text.vocab().most_common(1000)]\n",
        "\n",
        "train_x = [term_frequency(d) for d, _ in train_docs]\n",
        "test_x = [term_frequency(d) for d, _ in test_docs]\n",
        "train_y = [c for _, c in train_docs]\n",
        "test_y = [c for _, c in test_docs]\n",
        "\n",
        "x_train = np.asarray(train_x).astype('float32')\n",
        "x_test = np.asarray(test_x).astype('float32')\n",
        "\n",
        "y_train = np.asarray(train_y).astype('float32')\n",
        "y_test = np.asarray(test_y).astype('float32')\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(1000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "             loss=losses.binary_crossentropy,\n",
        "             metrics=[metrics.binary_accuracy])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "json_string = model.to_json()\n",
        "with open(\"movie-review-model.json\", \"w\") as json_file:\n",
        "  json_file.write(json_string)\n",
        "print(\"Saved model for json format\")\n",
        "\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved weight for .h5 format\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IfJwZKLBfT4",
        "colab_type": "code",
        "outputId": "8799b12b-2f77-4b65-9ddc-fabed090c30d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "from numpy import argmax\n",
        "from tensorflow.compat.v2.keras.models import load_model\n",
        "from tensorflow.compat.v2.keras.models import model_from_json\n",
        "\n",
        "\n",
        "def term_frequency(doc):\n",
        "    return [doc.count(word) for word in selected_words]\n",
        "\n",
        "\n",
        "def load_model():\n",
        "  if os.path.isfile('train_docs.json'):\n",
        "    json_file = open('movie-review-model.json', 'r')\n",
        "    model_json = json_file.read()\n",
        "    json_file.close()\n",
        "\n",
        "    loaded_model = model_from_json(model_json)\n",
        "    model.load_weights('model.h5')\n",
        "    return model\n",
        "  else:\n",
        "    raise ValueError('there is no file')\n",
        "\n",
        "\n",
        "def predict_postitive_rate(review):\n",
        "  model = load_model()\n",
        "  token = tokenize(review)\n",
        "  tf = term_frequency(token)\n",
        "  data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)\n",
        "  \n",
        "  score = float(model.predict(data))\n",
        "  if(score > 0.5):        \n",
        "    print(\"[{}]는 {:.2f}% 확률로 긍정적 리뷰입니다.\\n\".format(review, score * 100))\n",
        "  else:        \n",
        "    print(\"[{}]는 {:.2f}% 확률로 부정적 리뷰입니다.\\n\".format(review, (1 - score) * 100))\n",
        "\n",
        "\n",
        "predict_postitive_rate(\"인생 최고의 영화!\")\n",
        "predict_postitive_rate(\"최고의 배우. 그러나 답답한 스토리 전개. 배우가 아깝다.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[인생 최고의 영화!]는 94.49% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n",
            "\n",
            "[최고의 배우. 그러나 답답한 스토리 전개. 배우가 아깝다.]는 95.27% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
